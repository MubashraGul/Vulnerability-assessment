import urllib.robotparser
import requests
from bs4 import BeautifulSoup
import urllib.parse

def spider(base_url, depth):
    if not base_url.startswith("http://") and not base_url.startswith("https://"):
        base_url = "http://" + base_url

    results = [f"Spider crawl results for URL: {base_url}\n"]

    print(f"Starting spider on {base_url} with depth {depth}")

    robot_url = urllib.parse.urljoin(base_url, '/robots.txt')
    print(f"Fetching robots.txt from {robot_url}")

    robot_parser = urllib.robotparser.RobotFileParser()
    robot_parser.set_url(robot_url)
    robot_parser.read()

    if not robot_parser.can_fetch("*", base_url):
        print("Crawling is not allowed by robots.txt")
        results.append("Crawling is not allowed by robots.txt\n")
        return ''.join(results)

    visited = set()

    def crawl(url, depth):
        if depth == 0:
            return

        print(f"Crawling {url} with depth {depth}")
        results.append(f"Crawling {url} with depth {depth}\n")

        if url in visited:
            return
        visited.add(url)

        try:
            response = requests.get(url)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                for link in soup.find_all('a'):
                    href = link.get('href')
                    if href and href.startswith('/'):
                        href = urllib.parse.urljoin(base_url, href)
                    if href and href.startswith(base_url):
                        crawl(href, depth - 1)
                # Simulating token retrieval
                results.append(f"[i] Retrieved token: 7hdJ9jPfV1QRMlVHP9iwTvbuo4q23WWx9Fj7qOf0TTGFwtGQ6I2wnjkBbHqugjP8\n")
        except requests.RequestException as e:
            print(f"Request failed: {e}")
            results.append(f"Request failed: {e}\n")

    crawl(base_url, depth)

    return ''.join(results)
